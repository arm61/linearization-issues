% Define document class
\documentclass[journal=jceda8,manuscript=article]{achemso}
\usepackage{showyourwork}
\usepackage[version=4]{mhchem}
\usepackage{graphicx}
\usepackage{siunitx}
\sisetup{range-exponents = combine-bracket}

% Author list
\author{Andrew R. McCluskey}
\email{andrew.mccluskey@ess.eu}
\affiliation{European Spallation Source ERIC, Ole Maaløes vej 3, 2200 København N, DK}

% Title
\title{Is there still a place for linearization in the chemistry curriculum?}

% Begin!
\begin{document}

% Abstract with filler text
\begin{abstract}
    The use of mathematical transformations to reduce non-linear functions to linear problems that can be tackled with analytical linear regression is commonplace in chemistry textbooks and degree programs. 
    However, the linearisation procedure can lead to biased estimates of regression parameters, when real measured data is used. 
    Only introducing students to linearization, without discussion of the shortcomings, leads to researchers applying this biased process in formal analysis.
    Modern computing technology means that non-linear optimization is more accessible than ever. 
    I hope to start a discussion in the community as to the place of linearization, and more broadly the adequacy of the current training in data handling skills we offer to students.
\end{abstract}

\maketitle 

% Linerisation is a thing that features in chemistry degrees.
Non-linear relationships are commonly found between dependent and independent variables in chemistry. 
``Linearization'' is a popular solution to quantify such relationships, where some mathematical transformation is used to reduce the non-linear problem to a linear one. 
The relationship can then be quantified using the analytical linear regression, rather than the numerical optimisation required for non-linear fitting. 
We see this in first- and second-order rate equations, and the Clausius-Clapeyron and Arrhenius equations, to name just a few~\cite{perrin_linear_2017,harper_data_2017,monk_math_2010}.
The simplicity and utility of linearization has led to its appearance in chemistry textbooks~\cite{monk_math_2010,atkins_physical_2018} and undergraduate programs. 

% Linearization appears in research literature cause it is taught in degree programmes. 
The linearization process is mathematically sound for noise-free measurements. 
However, as we shall see, the use of linearization to reduce a non-linear function to a linear form, followed by analysis with linear regression, can lead to biased estimates of the regression parameters; the gradient and intercept of the straight line. 
Therefore, the use of linearization should be avoided in formal analysis, however, it is still regularly found in research publications. 
This is due to the continued inclusion of linearization in a general chemistry education, without discussion of its problems. 
Non-linear optimisation is now accessible, through standard analysis software and programming languages, and lacks the pitfalls of linearization. 

% Example of linearization to show problem -- background.
A simple example can show the bias that linearisation introduces in the estimates of regression parameters. 
Consider the decomposition of hydrogen peroxide \ce{H2O2} in the presence of excess cerium(III) ion, which follows first-order rate kinetics (this example has been taken from Monk and Munro~\cite{monk_math_2010})
%
\begin{equation}
    [\ce{H2O2}]_t = [\ce{H2O2}]_0\exp{(-kt)},
    \label{eqn:first}
\end{equation}
%
where, $[\ce{H2O2}]_t$ is the concentration of hydrogen peroxide at time $t$, $[\ce{H2O2}]_0$ is the initial concentration and $k$ is the rate constant (shown in the non-linear form with representative data in Fig.~\ref{fig:fit_first}a).
Linearization of Eqn.~\ref{eqn:first} involves taking the natural logarithm of both sides to produce
%
\begin{equation}
    \ln{[\ce{H2O2}]_t} = -kt + \ln{[\ce{H2O2}]_0}.
\end{equation}
%
The gradient and intercept from linear regression are therefore equal to $-k$ and $\ln{[\ce{A}]_0}$, respectively (Fig.~\ref{fig:fit_first}b).

% Example of linearization to show problem -- result.
If we are to perform repeated measurements (in the case of Fig.~\ref{fig:fit_first}, $2^{15}$ repeats) of the concentration of \ce{H2O2} as a function of reaction time and analyse each repeat with both the linearization process and non-linear fitting, a range of estimates of $k$ will be obtained from each (Fig.~\ref{fig:fit_first}c \&~\ref{fig:fit_first}d).
The non-linear fitting results in a normal distribution of estimated values of $k$, with a mean, normalised by true $k$, of \variable{output/non_mean.txt}, i.e. the estimation is unbiased, and a \SI{95}{\percent} confidence interval range of \variable{output/non_ci.txt}.
Using the linearized form gives a biased estimate of $k$, with a normalised mean of \variable{output/lin_mean.txt}, the magnitude of the bias increases with increasing noise in the data.
This non-normal distribution is also significantly broader than the non-linear optimisation estimate, with a \SI{95}{\percent} confidence interval range of \variable{output/lin_ci.txt}
This means that, on average, the linearized approach will overestimate the value of $k$ and estimate a value for $k$ further from the true value. 
%
\begin{figure}
  \includegraphics[width=0.5\columnwidth]{figures/wls.pdf}
  \caption{
    Representative data for first-order integrated rate equation, with a true value of $k=\SI{0.15}{\per\second}$ and $[\ce{A}]_0=\SI{7.5}{\mol\m^{-3}}$ and a constant uncertainty of \SI{0.3}{\mol\m^{-3}}, showing (a) the non-linear and (b) the linearized forms. 
    The estimate of $k$, normalised to the true value of $k$, from $2^{15}$ analyses of unique representative datasets, where $[\ce{A}]_0$ was fixed to the true value, using (c) non-linear fitting and (d) linearization, the vertical lines indicate the mean of the distribution. 
    }
  \label{fig:wls}
  \script{wls.py}
\end{figure}
%

% Linearization involves the transformation of a normal distribution to something non-normal.
We can understand the cause of the observed bias by recognising that the measurement of a dependent variable $y$ is only ever an estimate of the true value, $\hat{y}$, which is a random draw from a distribution of values, $P(y)$. 
The shape of this distribution depends on the noise or uncertainty in the measurement. 
It is commonly assumed that random uncertainty sources will lead to a normal distribution, $P(y) \sim \mathcal{N}(\mu, \sigma^2)$, which is defined by the mean, $\mu$, and standard deviation, $\sigma$ (Fig.~\ref{fig:distributions})~\cite{monk_math_2010}.
When linearization is used, some mathematical transformation is performed on the dependent variable and if that transformation scales in a non-linear fashion, i.e. the reciprocal or logarithm is taken, it will result in the normally distributed variable becoming non-normal (Figs.~\ref{fig:distributions}b \&~\ref{fig:distributions}c).
%
\begin{figure}
  \includegraphics[width=0.66 \columnwidth]{figures/distributions.pdf}
  \caption{
    Histograms showing the effect of non-linearly scaling mathematical transformations on (a) a normal distribution, $\mathcal{N}(50, 10^2)$, by taking (b) the reciprocal or (c) the logarithm. 
    Produced from $2^{15}$ random samples from the normal distribution.
    }
  \label{fig:distributions}
  \script{distributions.py}
\end{figure}
%

% Least squares/linear regression is unbiased where the dependent variable is normally distributed.
The standard deviation of $P(y)$ can often be estimated for a given measurement and used to weight measurements based on confidence, as noted by Perrin, this is particularly important in the case of linearized data, where constant uncertainty becomes heteroscedastic~\cite{perrin_linear_2017}.
For normally distributed variables, weighted linear regression produces unbiased estimates of the gradient and intercept. 
However, this is not the case when non-normally distributed variables are used, and for repeated measurements, there may be a systematic difference between the observed mean of the estimated parameter and its true value.
This is not the case when non-linear optimization by least squares is applied to normally distributed variables. 

% Linearization is bad, we should replace it with training in non-linear optimisation. 
The linearization process can lead to biased estimates of parameters of interest and therefore has no place in formal analysis. 
Yet, it is still taught regularly to chemistry students, due in part to the complexity of the more robust non-linear optimisation. 
In a classroom or exam hall, it is feasible for a student, equipped with graph paper and a ruler to estimate the gradient and intercept of a straight line. 
Additionally, the linearization process can be a valuable tool where quantitative analysis is not required to identify general trends in data. 

Recent developments in computing and access to programming with tools, such as the Jupyter Notebook~\cite{kluyver_jupyter_2016} and the Fit-o-mat program~\cite{mglich_open_2018}, mean that non-linear optimisation is more accessible than ever. 
Therefore, I believe that the deficiencies of linearization should be taught alongside the non-linear optimisation solution. 
In addition to reducing the use of the flawed linearization process in the chemical research literature, this will give students a more rounded understanding of robust data analysis, while keeping the utility that is possible for ``quick'' analyses with linearization.

\section*{Data availability}

Electronic Supplementary Information (ESI) available: A complete set of analysis/plotting scripts allowing for a fully reproducible and automated analysis workflow, using showyourwork~\cite{luger_showyourwork_2021}, for this work and a Jupyter Notebook showing the use of weighted non-linear optimisation for representative first-order rate kinetics data is available at \url{https://github.com/arm61/against-linearisation} (DOI: 10.5281/zenodo.xxxxxxx) under an MIT license, while the text is shared under a CC BY-SA 4.0 license~\cite{mccluskey_github_2023}.

\section*{Acknowledgements}

The author thanks Benjamin J. Morgan, Samuel W. Coles, Thomas Holm Rod, Gabriel Krenzer, and Kasper Tolborg for the insightful discussion that lead to this work. 
Additionally, the author would like to thank those that engaged in discussion on Twitter when the problem of linearization in Arrhenius modelling was initially raised. 

\bibliography{bib}

\end{document}
