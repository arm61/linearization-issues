% Define document class
\documentclass[journal=jceda8,manuscript=article]{achemso}
\usepackage{showyourwork}
\usepackage[version=4]{mhchem}
\usepackage{graphicx}
\usepackage{siunitx}
\sisetup{range-exponents = combine-bracket}

% Author list
\author{Andrew R. McCluskey}
\email{andrew.mccluskey@ess.eu}
\affiliation{European Spallation Source ERIC, Ole Maaløes vej 3, 2200 København N, DK}

% Title
\title{Is there still a place for linearization in the chemistry curriculum?}

% Begin!
\begin{document}

% Abstract with filler text
\begin{abstract}
    The use of mathematical transformations to reduce non-linear functions to linear problems that can be tackled with analytical linear regression is commonplace in the chemistry curriculum. 
    The linearization procedure, however, assumes an incorrect statistical model for real experimental data; leading to biased estimates of regression parameters and inaccurate uncertainty propagation through the analysis process. 
    Modern computing technology means that non-linear optimization is more accessible than ever. 
    So, by introducing students to linearization, without a detailed discussion of the shortcomings, we are failing to equip students with correct tools for formal data analysis. 
    I hope that this commentary will start a discussion in the community as to the place of linearization in the chemistry curriculum. 
\end{abstract}

\maketitle 

% Linerisation is a thing that features in chemistry degrees.
Non-linear relationships are commonly found between dependent and independent variables in chemistry.
These relationships can be simplified by the process of ``linearization'', where some mathematical transformation is used to reduce the non-linear problem to a linear one. 
By linearizing the function, analytical linear regression can be used to quantify parameters of interest, rather than relying on numerical optimisation. 
We see this process in chemistry textbooks~\cite{monk_math_2010,atkins_physical_2018} and undergraduate programs, where it is applied to first- and second-order rate equations, and the Clausius-Clapeyron and Arrhenius equations, to name just a few~\cite{perrin_linear_2017,harper_data_2017,monk_math_2010}.

% Linearization appears in research literature cause it is taught in degree programmes. 
While mathematically sound for noise-free measurements, linearization can introduce errors in the analysis process for real experimental data. 
Specifically, it can lead to biased estimates of regression parameters; the gradient and intercept of the straight line, as well as, inaccurate estimates of their uncertainty. 
Therefore, the use of linearization should be avoided in formal analysis.
Because linearization is included in a general chemistry education, without discussion of its problems, it is still regularly found in research publications. 
Although not analytically tractable, non-linear optimisation is now accessible, through standard analysis software and programming languages, and lacks the pitfalls of linearization. 

% Example of linearization to show problem -- background.
To show the problems of linearization, we can consider the decomposition of hydrogen peroxide \ce{H2O2} in the presence of excess cerium(III) ion, which follows first-order rate kinetics~\cite{monk_math_2010}
%
\begin{equation}
    [\ce{H2O2}]_t = [\ce{H2O2}]_0\exp{(-kt)},
    \label{eqn:first}
\end{equation}
%
where, $[\ce{H2O2}]_t$ is the concentration of hydrogen peroxide at time $t$, $[\ce{H2O2}]_0$ is the initial concentration and $k$ is the rate constant (shown in the non-linear form with representative data in Fig.~\ref{fig:ols}a).
Linearization of Eqn.~\ref{eqn:first} involves taking the natural logarithm of both sides to produce
%
\begin{equation}
    \ln{[\ce{H2O2}]_t} = -kt + \ln{[\ce{H2O2}]_0}.
    \label{eqn:log}
\end{equation}
%
The gradient and intercept from linear regression are therefore equal to $-k$ and $\ln{[\ce{A}]_0}$, respectively (Fig.~\ref{fig:ols}b).

% Example of linearization to show problem -- result.
If we were to perform repeated measurements of the concentration of \ce{H2O2} as a function of reaction time and analyse each repeat, we can build up a distribution of estimates of $k$ (Fig.~\ref{fig:ols}c \&~\ref{fig:ols}d). 
Either linearization of the data followed by ordinary least squares (OLS) regression or non-weighted non-linear optimization can be used. 
Non-linear fitting results in a normal distribution of estimated values of $k$, with a mean, normalised by the true $k$, of \variable{output/non_mean_ols.txt}, i.e., the estimation is unbiased. 
The linearized form, however, gives a biased estimate of $k$, with a normalised mean of \variable{output/lin_mean_ols.txt}, the magnitude of the bias increases with increasing noise in the data.
Additionally, the distribution of values from the linearized from is significantly broader (a \SI{95}{\percent} confidence interval range of \variable{output/lin_ci_ols.txt}) than the non-linear result (\SI{95}{\percent} confidence interval range of \variable{output/non_ci_ols.txt}).
This means that, on average, the linearized approach will overestimate the value of $k$ and estimate a value for $k$ further from the true value. 
%
\begin{figure}
  \includegraphics[width=0.5\columnwidth]{figures/ols.pdf}
  \caption{
    Representative data for first-order integrated rate equation, with a true value of $k=\SI{0.15}{\per\second}$ and $[\ce{A}]_0=\SI{7.5}{\mol\m^{-3}}$, showing (a) the non-linear and (b) the linearized forms. 
    The estimate of $k$, normalised to the true value of $k$, from $2^{15}$ analyses of unique representative datasets, where $[\ce{A}]_0$ was fixed to the true value, using (c) non-linear fitting and (d) linearization, the vertical lines indicate the mean of the distribution. 
    }
  \label{fig:ols}
  \script{ols.py}
\end{figure}
%

Through the use of OLS or non-weighted optimisation we are assuming that the uncertainties in our data are all the same, i.e., they are homoscedastic. 
It was noted by Perrin~\cite{perrin_linear_2017}, however, that homoscedastic uncertainties may become heteroscedastic as a result of the linearization process (note the error bars in Fig.~\ref{fig:wls}b). 
Therefore, the use of OLS for linearized data is insufficient, instead we should accurately propagate the measured uncertainty, for the example in Eqn.~\ref{eqn:log} the propagated error is the measured error divided by the nominal value (Fig.~\ref{fig:wls}b), and use weighted least squares (WLS).


It was previously noted by Perrin~\cite{perrin_linear_2017} that the linearisation process will produce uncertainty that is not the same for all values. 
In the above example, uncertainty in the non-linear model should be propagated by dividing each by the nominal value, producing heteroscedastic uncertainties. 
Therefore, the use of OLS for the linearized data is insufficient, instead we should accurately propagate the measured uncertainty in our measurement, i.e. $\mathrm{d}y = \mathrm{d}x / x$, and perform weighted least squares (WLS). 
This leads to significant improvement in the distribution of $\hat{k}$, with an unbiased normal distribution for both the weighted non-linear optimization and WLS approaches (Figs.~\ref{fig:wls}c \&~\ref{fig:wls}d).
This 


% However, this reveals a different problem, from each analysis, we can estimated the uncertainty in $\hat{k}$, and using WLS we significantly overestimate this. 
% The true uncertainty, the width of the solid distribution, using WLS is found to be \variable{output/lin_err_true.txt}, however the mean estimated uncertainty was \variable{output/lin_err.txt}. 
% We compare this with the result from the non-linear approach, where the true uncertainty is smaller than the WLS approach (i.e., more precise) with a value of \variable{output/non_err_true.txt} and the mean estimated value was \variable{output/non_err.txt}.
%
\begin{figure}
  \includegraphics[width=0.5\columnwidth]{figures/wls.pdf}
  \caption{
    Representative data for first-order integrated rate equation, with a true value of $k=\SI{0.15}{\per\second}$ and $[\ce{A}]_0=\SI{7.5}{\mol\m^{-3}}$, showing (a) the non-linear and (b) the linearized forms. 
    The estimate of $k$, normalised to the true value of $k$, from $2^{15}$ analyses of unique representative datasets, where $[\ce{A}]_0$ was fixed to the true value, using (c) non-linear fitting and (d) linearization, the vertical lines indicate the mean of the distribution. 
    }
  \label{fig:wls}
  \script{wls.py}
\end{figure}
%


% Linearization involves the transformation of a normal distribution to something non-normal.
We can understand the cause of the observed bias by recognising that the measurement of a dependent variable $y$ is only ever an estimate of the true value, $\hat{y}$, which is a random draw from a distribution of values, $P(y)$. 
The shape of this distribution depends on the noise or uncertainty in the measurement. 
It is commonly assumed that random uncertainty sources will lead to a normal distribution, $P(y) \sim \mathcal{N}(\mu, \sigma^2)$, which is defined by the mean, $\mu$, and standard deviation, $\sigma$ (Fig.~\ref{fig:distributions})~\cite{monk_math_2010}.
When linearization is used, some mathematical transformation is performed on the dependent variable and if that transformation scales in a non-linear fashion, i.e. the reciprocal or logarithm is taken, it will result in the normally distributed variable becoming non-normal (Figs.~\ref{fig:distributions}b \&~\ref{fig:distributions}c).
%
\begin{figure}
  \includegraphics[width=0.66 \columnwidth]{figures/distributions.pdf}
  \caption{
    Histograms showing the effect of non-linearly scaling mathematical transformations on (a) a normal distribution, $\mathcal{N}(50, 10^2)$, by taking (b) the reciprocal or (c) the logarithm. 
    Produced from $2^{15}$ random samples from the normal distribution.
    }
  \label{fig:distributions}
  \script{distributions.py}
\end{figure}
%

% Least squares/linear regression is unbiased where the dependent variable is normally distributed.
The standard deviation of $P(y)$ can often be estimated for a given measurement and used to weight measurements based on confidence, as noted by Perrin, this is particularly important in the case of linearized data, where constant uncertainty becomes heteroscedastic~\cite{perrin_linear_2017}.
For normally distributed variables, weighted linear regression produces unbiased estimates of the gradient and intercept. 
However, this is not the case when non-normally distributed variables are used, and for repeated measurements, there may be a systematic difference between the observed mean of the estimated parameter and its true value.
This is not the case when non-linear optimization by least squares is applied to normally distributed variables. 

% Linearization is bad, we should replace it with training in non-linear optimisation. 
The linearization process can lead to biased estimates of parameters of interest and therefore has no place in formal analysis. 
Yet, it is still taught regularly to chemistry students, due in part to the complexity of the more robust non-linear optimisation. 
In a classroom or exam hall, it is feasible for a student, equipped with graph paper and a ruler to estimate the gradient and intercept of a straight line. 
Additionally, the linearization process can be a valuable tool where quantitative analysis is not required to identify general trends in data. 

Recent developments in computing and access to programming with tools, such as the Jupyter Notebook~\cite{kluyver_jupyter_2016} and the Fit-o-mat program~\cite{mglich_open_2018}, mean that non-linear optimisation is more accessible than ever. 
Therefore, I believe that the deficiencies of linearization should be taught alongside the non-linear optimisation solution. 
In addition to reducing the use of the flawed linearization process in the chemical research literature, this will give students a more rounded understanding of robust data analysis, while keeping the utility that is possible for ``quick'' analyses with linearization.

\section*{Data availability}

Electronic Supplementary Information (ESI) available: A complete set of analysis/plotting scripts allowing for a fully reproducible and automated analysis workflow, using showyourwork~\cite{luger_showyourwork_2021}, for this work and a Jupyter Notebook showing the use of weighted non-linear optimisation for representative first-order rate kinetics data is available at \url{https://github.com/arm61/against-linearisation} (DOI: 10.5281/zenodo.xxxxxxx) under an MIT license, while the text is shared under a CC BY-SA 4.0 license~\cite{mccluskey_github_2023}.

\section*{Acknowledgements}

The author thanks Benjamin J. Morgan, Samuel W. Coles, Thomas Holm Rod, Gabriel Krenzer, and Kasper Tolborg for the insightful discussion that lead to this work. 
Additionally, the author would like to thank those that engaged in discussion on Twitter when the problem of linearization in Arrhenius modelling was initially raised. 

\bibliography{bib}

\end{document}
